{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "micro-attention",
   "metadata": {},
   "source": [
    "# Intro\n",
    "在AI的数据部分，主要处理以下任务：\n",
    "1. 数据收集：收集样本和对应标签（可能是整个AI流程中最麻烦的一部分）\n",
    "2. 数据划分：将数据划分为训练集，测试集，验证集\n",
    "3. 数据读取：数据读取要求给定一个索引，能返回一个样本及标签，即index => （data，label)这样的一个映射，在torch中，对应的实现模块为DataLoader，其中包含一个Sampler和DataSet，其中Sampler负责生成索引，DataSet根据索引返回样本及标签\n",
    "4. 数据预处理：将数据输入转化为模型要求的输入格式，对应于torch的transform模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-montreal",
   "metadata": {},
   "source": [
    "# 数据收集\n",
    "这一部分工作，一般借助爬虫等工具完成，还有数据清理等工作比较琐碎，不予介绍，在这里我们使用RMB_data\n",
    "\n",
    "通常而言，数据收集完后交付的数据集一般是如下形式，即不同文件夹对应不同label的数据\n",
    "```shell\n",
    "-- XXX_data\n",
    "    | -- label_1\n",
    "        | -- a.png\n",
    "        | -- b.png\n",
    "        | -- c.png\n",
    "    | -- label_2\n",
    "        | -- x.png\n",
    "        | -- y.png\n",
    "        | -- z.png\n",
    "```\n",
    "\n",
    "其他形式例如：xxx.csv等"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-scale",
   "metadata": {},
   "source": [
    "# 数据划分\n",
    "数据划分的任务就是划分数据为train, valid, test三个集合，划分如下：\n",
    "```shell\n",
    "-- XXX_split\n",
    "    | -- train\n",
    "        | -- label_1\n",
    "            | -- a.png\n",
    "            | -- b.png\n",
    "        | -- label_2\n",
    "    | -- valid\n",
    "        | -- label_1\n",
    "        | -- label_2\n",
    "    | -- test\n",
    "        | -- label_1\n",
    "        | -- label_2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italic-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "restricted-glucose",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 100, train: 80, valid: 10, test: 10\n",
      "Class: 1, train: 80, valid: 10, test: 10\n"
     ]
    }
   ],
   "source": [
    "def makedir(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "        \n",
    "random.seed(1)\n",
    "\n",
    "dataset_dir = os.path.join(\"data\", \"RMB_data\")\n",
    "split_dir = os.path.join(\"data\", \"rmb_split\")\n",
    "\n",
    "train_dir = os.path.join(split_dir, \"train\")\n",
    "valid_dir = os.path.join(split_dir, \"valid\")\n",
    "test_dir = os.path.join(split_dir, \"test\")\n",
    "\n",
    "train_pct = 0.8\n",
    "valid_pct = 0.1\n",
    "test_pct = 0.1\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for sub_dir in dirs:\n",
    "        imgs = os.listdir(os.path.join(root, sub_dir))\n",
    "        imgs = list(filter(lambda x: x.endswith('.jpg'), imgs))\n",
    "        random.shuffle(imgs)\n",
    "        img_count = len(imgs)\n",
    "        train_point = int(img_count * train_pct)\n",
    "        valid_point = int(img_count * (train_pct + valid_pct))\n",
    "        for i in range(img_count):\n",
    "            if i < train_point:\n",
    "                out_dir = os.path.join(train_dir, sub_dir)\n",
    "            elif i < valid_point:\n",
    "                out_dir = os.path.join(valid_dir, sub_dir)\n",
    "            else:\n",
    "                out_dir = os.path.join(test_dir, sub_dir)\n",
    "            makedir(out_dir)\n",
    "            target_path = os.path.join(out_dir, imgs[i])\n",
    "            src_path = os.path.join(dataset_dir, sub_dir, imgs[i])\n",
    "            shutil.copy(src_path, target_path)\n",
    "        print('Class: {}, train: {}, valid: {}, test: {}'.format(sub_dir, train_point, valid_point-train_point, img_count-valid_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-athens",
   "metadata": {},
   "source": [
    "# 数据读取\n",
    "在torch中，数据读取范式为DataLoader，在实现DataLoader之前，需要先实现Dataset，Dataset根据索引返回样本及标签（在__getitem__中实现)\n",
    "\n",
    "Dataset: indices => (data, label)\n",
    "\n",
    "DataLoader实现如下：\n",
    "DataLoader(Dataset, batch_size=BATCH_SIZE, shuffle=True/False)\n",
    "\n",
    "在使用时：\n",
    "```python\n",
    "for inputs, labels in train_loader:\n",
    "    ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "frank-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subtle-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "rmb_label = {\"1\": 0, \"100\": 1}\n",
    "\n",
    "class RMBDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "            self.data_info为一个list，因此可以通过它知道其有多少个元素\n",
    "            在这里是[(img_path, int(label))]\n",
    "        \"\"\"\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path, label = self.data_info[index]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        for root, dirs, _ in os.walk(data_dir):\n",
    "            for sub_dir in dirs:\n",
    "                img_names = os.listdir(os.path.join(root, sub_dir))\n",
    "                img_names = list(filter(lambda x: x.endswith('.jpg'), img_names))\n",
    "                for i in range(len(img_names)):\n",
    "                    img_name = img_names[i]\n",
    "                    img_path = os.path.join(root, sub_dir, img_name)\n",
    "                    label = rmb_label[sub_dir]\n",
    "                    data_info.append((img_path, int(label)))\n",
    "        return data_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-birthday",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    "transform模块使用如下：\n",
    "```python\n",
    "# 设置训练集的数据增强和转化\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 设置验证集的数据增强和转化，不需要 RandomCrop\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_data = RMBDataset(data_dir=train_dir, transform=train_transform)\n",
    "valid_data = RMBDataset(data_dir=valid_dir, transform=valid_transform)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
