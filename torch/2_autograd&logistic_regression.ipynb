{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sixth-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "former-status",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7b70428af0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-force",
   "metadata": {},
   "source": [
    "# intro\n",
    "机器学习的算法具有非常通用的编程范式(Paradigm)，\n",
    "\n",
    "1. data\n",
    "2. model\n",
    "3. target \n",
    "4. algorithm\n",
    "5. visualization\n",
    "\n",
    "因此，对于不同的算法，我们只需要在范式内修改即可，\n",
    "\n",
    "因此，机器学习的算法实现并不具有技巧性，难点在于数学层面的推导和理解\n",
    "\n",
    "下面给出机器学习算法的范式，在后面我们还会针对数据的加载和可视化两个部分进行优化\n",
    "1. 如何加载数据，并以tensor的形式输入到模型？\n",
    "2. 如何动态可视化训练过程的loss，acc等？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-thailand",
   "metadata": {},
   "source": [
    "# 1. 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "celtic-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_nums = 100\n",
    "mean_value = 1.7\n",
    "bias = 1\n",
    "n_data = torch.ones(sample_nums, 2) \n",
    "\n",
    "# use normal distribution to generate training samples\n",
    "x0 = torch.normal(mean_value *  n_data, 1) + bias # 在(1, 1.7)处生成normal分布的数据\n",
    "y0 = torch.zeros(sample_nums) # labeled as 0\n",
    "\n",
    "x1 = torch.normal(-mean_value * n_data, 1) + bias\n",
    "y1 = torch.ones(sample_nums)\n",
    "\n",
    "train_x = torch.cat((x0, x1), 0) # T x 2 \n",
    "train_y = torch.cat((y0, y1), 0) # T x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-backing",
   "metadata": {},
   "source": [
    "# 2. 模型\n",
    "模型就是一个从输入到输出的带参映射\n",
    "\n",
    "这里nn.Linear的参数由weights和bias组成，其中weights和bias的ndim都是1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "based-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "            在__init__中描述模型的building blocks有哪些，所有的building blocks应该在这里引入\n",
    "        \"\"\"\n",
    "        super(LR, self).__init__()\n",
    "        self.features = nn.Linear(2, 1) # 2 -> 1, 1x2矩阵，包含两个参数，分别是weight和bias\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            在forward中描述模型的计算过程，即如何从输入得到输出\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "lr_net = LR() # Tx2 -(Linear) -> Tx1 -(Sigmoid) -> Tx1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-farming",
   "metadata": {},
   "source": [
    "# 3. 目标/损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-nursing",
   "metadata": {},
   "source": [
    "# 4. 算法 & 可视化过程\n",
    "值得注意的是这里计算acc的方式比较tricky:\n",
    "```python\n",
    "mask = y_pred.ge(0.5).float().squeeze()\n",
    "correct = (mask == train_y).sum()\n",
    "acc = correct.item() / train_y.size()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "third-nylon",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a1dd86fa231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# y_pred = model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(lr_net.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "for iteration in range(1000):\n",
    "    # forward: y_pred = model(x)\n",
    "    y_pred = lr_net(train_x)\n",
    "    # loss\n",
    "    loss = loss_fn(y_pred.squeeze(), train_y) # y_pred包含创建它的函数信息，因此能向前追溯，进行反向传播\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    # update parameter\n",
    "    optimizer.step()\n",
    "    # clear old grads\n",
    "    optimizer.zero_grad()\n",
    "    # drawing \n",
    "    if iteration % 20 == 0:\n",
    "        mask = y_pred.ge(0.5).float().squeeze()\n",
    "        correct = (mask == train_y).sum()\n",
    "        acc = correct.item() / train_y.size(0)\n",
    "        \n",
    "        plt.scatter(x0.data.numpy()[:, 0], x0.data.numpy()[:, 1], c='r', label='class 0')\n",
    "        plt.scatter(x1.data.numpy()[:, 0], x1.data.numpy()[:, 1], c='b', label='class 1')\n",
    "        \n",
    "        w0, w1 = lr_net.features.weight[0]\n",
    "        w0, w1 = float(w0.item()), float(w1.item())\n",
    "        plot_b = float(lr_net.features.bias[0].item())\n",
    "        plot_x = np.arange(-6, 6, 0.1)\n",
    "        plot_y = (-w0 * plot_x - plot_b) / w1\n",
    "        \n",
    "        plt.xlim(-5, 7)\n",
    "        plt.ylim(-7, 7)\n",
    "        plt.plot(plot_x, plot_y)\n",
    "        \n",
    "        plt.text(-5, 5, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color': 'red'})\n",
    "        plt.title(\"Iteration: {}\\nw0:{:.2f} w1:{:.2f} b: {:.2f} accuracy:{:.2%}\".format(iteration, w0, w1, plot_b, acc))\n",
    "        plt.legend()\n",
    "        # plt.savefig(str(iteration / 20)+\".png\")\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        # 如果准确率大于 99%，则停止训练\n",
    "        if acc > 0.99:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
